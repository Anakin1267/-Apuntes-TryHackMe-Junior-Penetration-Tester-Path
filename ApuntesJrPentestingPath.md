# <span style="color:green;">**JUNIOR PENTESTING PATH - THM**</span>  

> Esto van a ser unos apuntes de repaso sobre la ruta de aprendizaje de TryHackMe "Jr Penetraition Tester", aquí trataremos sobre todo aquellos módulos interesantes pero que quizás estan poco explicados y detallados en el curso, así además, aproechamos para recordar nombres de herramientas, comandos, siglas etc. Igual que aquí muchos módulos están explicados a fondo, muchos otros ni aparecen ya que para mi criterio no cobraban tanto protagonismo e interes.  

> <span style="color:skyblue;">**Azul clarito**</span> = Nombre del Módulo  
<span style="color:blue;">**Azul marino**</span> = Nombre de los Submódulos  
<span style="color:purple;">**Morado**</span> = Nombre de las tareas  

<br>

# <span style="color:skyblue;"> **INTRODUCCIÓN HACKING WEB**</span>  

## <span style="color:blue;">**DESCUBRIMIENTO DE CONTENIDO**</span>  

### <span style="color:purple;"> ****_ROBOTS.TXT y SITEMAP.XML_****</span>  

1. **robots.txt**
- Es un archivo de texto que se encuentra en el directorio raíz de un sitio web.
- Su función es dar instrucciones a los motores de búsqueda (bots) sobre qué páginas o secciones del sitio pueden o no deben ser rastreadas o indexadas.
- Utiliza reglas como `User-agent` (para especificar el bot) y `Disallow` (para bloquear secciones específicas del sitio).
- No impide que las páginas sean accesibles, simplemente evita que los motores de búsqueda las rastreen e indexen.

2. **sitemap.xml**
- Es un archivo XML que también se encuentra en el directorio raíz de un sitio web.
- Su función es listar todas las páginas importantes del sitio y proporcionar información adicional como:
  - Fecha de última modificación.
  - Frecuencia de actualización.
  - Prioridad de las páginas.
- Ayuda a los motores de búsqueda a rastrear e indexar el contenido más eficientemente, asegurando que ninguna página importante se quede fuera.  

> Ambas rutas son accesibles publicamente en la mayoria de sitios web

3. **Uso en pentesting de robots.txt**
- _Identificar áreas sensibles_: Al revisar el archivo robots.txt, los pentesters pueden descubrir qué partes del sitio están ocultas para los motores de búsqueda, lo que puede indicar áreas sensibles o confidenciales. Ejemplos comunes pueden incluir rutas como /admin/, /private/ o /backup/, que podrían contener información crítica.

- _Acceso directo_: Aunque robots.txt bloquea el rastreo, no bloquea el acceso directo. Un pentester podría intentar acceder manualmente a las URLs bloqueadas en busca de vulnerabilidades.

4. **Uso en pentesting de sitemap.xml**
- _Mapeo completo del sitio_: Un pentester puede usar el archivo sitemap.xml con aquellas URLs que podrían no estar bien enlazadas desde otras partes del sitio.

- _Prioridades del contenido_: Al ver qué páginas tienen alta prioridad y cuáles se actualizan frecuentemente, un pentester puede identificar áreas donde los desarrolladores están poniendo más esfuerzo o que son más críticas para el negocio, y enfocar sus pruebas allí.  

### <span style="color:purple;"> ****_FAVICON_****</span>  

El favicon es un pequeño ícono que se muestra en la pestaña del navegador y se utiliza para dar estilo y diferenciar un sitio web. Es un archivo `.ico` configurado en el **_head_** del HTML.  

Aunque a primera vista parezca inutil en cuanto al ámbito de pentesting, no es así. A veces cuando un framework es usado para el desarrolo de un sito web, si el desarrolador se olvida de remplazarlo, se quedará el favicon por defecto de dicho framework, lo cual nos daría información de este.  

> Un framework es una estructura o conjunto de herramientas y librerías predefinidas que facilita el desarrollo de aplicaciones al proporcionar soluciones reutilizables para tareas comunes.  

Haciendo un `Ctrl + u` examinamos el código de la págima web donde estamos, arriba en el **_head_** encontraremos una línea donde veremos algo así como `<link rel="shortcut icon" type=images/favicon.ico/>`, claro está que en cada página el nombre del archivo y su ruta será diferente, pero con saber la ruta nos sirve.  

Lo que vamos a hacer a continuacioón es usar la herramienta `curl` (Client URL) para descargarnos el archivo .ico, más tarde explicaré la razón de esto. En la consola de kali ejecutamos `curl https://ejemplo.com/favicon.ico | md5sum`  

Lo que hemos hecho ha sido descargarnos el archivo .ico y obtener su valor **_hash md5_**. El valor hash MD5 es una cadena de caracteres generada a partir de un algoritmo de resumen criptográfico (MD5). 

  - **_curl_** : Este comando utiliza curl para descargar el archivo favicon.ico desde el sitio web ejemplo.com  
  - **|** : Es el simbolo de _pipe_ que toma la salida del comando `curl` y la pasa como entrada al siguiente comando (md5sum)
  - **_md5sum_** : Este comando genera un hash MD5 a partir del contenido del archivo descargado  

Pero...¿Para qué queremos este hash?, el sitio web _OWASP_ (Open Web Application Service Project) cuenta con una base de datos de los hashes _md5_ de muchisimos frameworks, https://wiki.owasp.org/index.php/OWASP_favicon_database. Introduciendo el hash que tengamos en la base dato, podremos encontrar información posiblemente util para nuestros ataques de pentesting.

No es lo más interesante quizás pero desde luego es muy util en casos especificos.  

### <span style="color:purple;">**_ENCABEZADOS HTTP_**</span>  

<span style="color: khaki;">**1. ¿Qué son los encabezados HTTP?**</span>


> Los encabezados HTTP son cadenas de texto que se envían entre el cliente y el servidor para proporcionar información adicional sobre la solicitud o la respuesta. Hay dos tipos principales de encabezados:

- **Encabezados de solicitud (Request headers)**: Son enviados por el cliente al servidor e incluyen información como el agente de usuario, el tipo de contenido aceptado, las cookies, etc.  

- **Encabezados de respuesta (Response headers)**: Son enviados por el servidor al cliente e incluyen detalles como el tipo de contenido devuelto, cookies, políticas de cacheo, información del servidor, etc.  

<span style="color: khaki;">**2. ¿Para qué sirven los encabezados HTTP?**</span>

- **Controlar el comportamiento de la conexión**: Los encabezados permiten ajustar cómo debe manejarse la conexión, como por ejemplo especificar el tipo de contenido que el cliente espera recibir (`Content-Type`) o el tipo de datos aceptados (`Accept`).  

- **Autenticación y gestión de sesiones**: Los encabezados también gestionan la autenticación y las cookies, que son esenciales para mantener sesiones de usuario activas.  

- **Seguridad**: Los encabezados como `Strict-Transport-Security (HSTS)` fuerzan el uso de HTTPS para evitar ataques de intermediarios.  

- **Caching**: Controlan cómo los navegadores deben manejar el contenido en caché (con encabezados como `Cache-Control`, `Expires`).  
 
<span style="color: khaki;">**3. Ejemplo básico**</span>

Para poner un ejemplo de como se ven los encabezados y como son, usaré la herramienta `Curl` anteriormente usada. Es importante enntender que cada página web va a mostrar lineas de codigo de encabezados muy diferentes, mas o menos extensas etc. Siempre es muy util consultar en algun navegador o a alguna IA para entender linea a liena los encabezados.  

`curl https://ejemplo.com -v` es el comando que usaremos con cualquier URL que queramos.  

El comando realiza una solicitud HTTP a la URL especificada, mostrando la conversación completa entre el cliente y el servidor en el proceso.

- **`Curl`**: es una herramienta de línea de comandos que se usa para transferir datos utilizando diferentes protocolos (como HTTP, HTTPS, FTP, etc.).  

- **`https://ejemplo.com`**: Es la URL que vamos a analizar, no tiene porque ser la ruta principal, tambien podriamos poner una ruta más especifica como por ejemplo _`https://ejemplo.com/clientes/premium`_

- **`-v`**: (verbose) es una opción que permite ver información detallada sobre la transacción. Esto incluye todos los encabezados HTTP que se envían y reciben durante la solicitud y respuesta.  

Al ejecutar curl <url> -v, verás algo como esto:  

```
> GET / HTTP/1.1
> Host: example.com
> User-Agent: curl/7.81.0
> Accept: */*

< HTTP/1.1 200 OK
< Date: Thu, 24 Oct 2024 15:00:00 GMT
< Content-Type: text/html; charset=UTF-8
< Server: Apache
< Set-Cookie: sessionID=abcdef; HttpOnly
< Cache-Control: no-cache
```  

Este resultado muestra los encabezados que envía el cliente (`GET`, `Host`, `User-Agent`, `Accept`) y los encabezados que responde el servidor (`Content-Type`, `Server`, `Set-Cookie`, etc.).

 <span style="color: khaki;">**4. ¿Como es esto util en el Pentesting y Hacking Web?**</span>  

 En el contexto de pentesting y hacking web, los encabezados HTTP pueden revelar información crítica sobre la seguridad del sitio web y el comportamiento del servidor. Aquí te explico algunos usos clave:  

- **Identificación del servidor y software**: Encabezados como `Server` o `X-Powered-By` revelan la tecnología que utiliza el servidor (por ejemplo, Apache, Nginx, PHP, etc.). Esta información puede ayudar a identificar posibles vulnerabilidades específicas de esas tecnologías. Ejemplo:  

```
< Server: Apache/2.4.41 (Ubuntu)
< X-Powered-By: PHP/7.4.3
```  

- **Cookies y gestión de sesiones**: Los encabezados como `Set-Cookie` pueden revelar información sobre cómo el servidor gestiona las sesiones. Un mal manejo de cookies puede abrir la puerta a ataques como Session Hijacking o Cross-Site Scripting (XSS) si las cookies no están correctamente protegidas con las banderas `HttpOnly` o `Secure`. Ejemplo:  

```
< Set-Cookie: sessionID=abcdef; HttpOnly
```

- **Políticas de seguridad**: Los encabezados de seguridad como `Content-Security-Policy` (CSP), `X-Frame-Options` o `Strict-Transport-Security` indican si el sitio web está protegido contra ataques como Clickjacking, XSS o ataques de intermediario (MiTM). Ejemplo:  

```
< Strict-Transport-Security: max-age=63072000; includeSubDomains; preload
```

- **Cache-Control y Expires**: Si los encabezados de caché no están configurados correctamente, podrías encontrar problemas de exposición de datos sensibles o posibilidad de atacar mediante técnicas de cache poisoning. Ejemplo:  

```
< Cache-Control: no-cache
```

- **Redirecciones y el encabezado _Location_**: Puedes detectar redirecciones HTTP automáticas que podrían ser útiles para realizar ataques de phishing o redireccionamiento abierto (open redirect). Ejemplo:  

```
< HTTP/1.1 301 Moved Permanently
< Location: https://new-site.com/
```

- **Comportamiento del navegador**: Puedes manipular o analizar cómo el servidor espera que los navegadores traten ciertas solicitudes o respuestas, abriendo la puerta a vectores de ataque como Cross-Origin Resource Sharing (CORS).


### <span style="color:purple;">**_GOOGLE DORKING_**</span>  

<span style="color: khaki;">**1. ¿Qué es Google Dorking?**</span>  

Google Dorking es una técnica de hacking que utiliza operadores avanzados de búsqueda de Google para encontrar información específica y a veces sensible en sitios web, como archivos privados, bases de datos, o incluso credenciales que deberían estar protegidas. 

Esta práctica se basa en el uso de los "dorks", consultas de búsqueda específicas que permiten a un atacante descubrir vulnerabilidades o datos expuestos sin necesidad de hackear directamente un sistema.  

<span style="color: khaki;">**2. Ejemplos de Google Dorks y su Explicación**</span> 

**Encontrar archivos de un tipo específico:**

- _Dork:_  `site:ejemplo.com filetype:pdf` 

- Explicación: Este comando buscará todos los archivos PDF relacionados o con el nombre del dominio web que hayamos escrito. Esta técnica podría revelar documentos sensibles que deberían estar protegidos, pero que han sido indexados accidentalmente por Google.

- Posdata: Este comando se puede conmbinar con una palabra entre comillas despues del tipo de archivo; `site:ejemplo filetype:pdf "confidencial"`, esto hará una busqueda de PDFs con el nombre o la palabra "confidencial" relacionados al dominio web "ejemplo.com"  

**Encontrar chats y logs:**

- _Dork:_  `"index of" / "chat/logs"` 

- Explicación: Este comando buscará en el sitio web especificado directorios abiertos que contengan logs de chat. Los directorios llamados "chat/logs" podrían almacenar registros de conversaciones o información relacionada con sesiones de chat que, si no están adecuadamente protegidos, pueden ser accesibles. Estos archivos de log podrían incluir datos sensibles de usuarios, conversaciones privadas o información de soporte, lo cual puede ser un riesgo de privacidad y seguridad.

- Posdata: Este dork se puede modificar para buscar otros tipos de directorios específicos en un dominio en particular, ajustando las palabras clave en la búsqueda (por ejemplo, site:ejemplo.com "index of" / "private/logs"). Esto puede revelar archivos de registro adicionales o carpetas mal configuradas.


**Encontrar cámaras de vigilancia accesibles:**

- _Dork:_ `inurl:"/view.shtml"`

- Explicación: Este comando busca URL que contienen el término /view.shtml, que comúnmente se asocia con páginas de visualización de cámaras de vigilancia. Muchas cámaras IP y sistemas de seguridad utilizan esta estructura de URL para permitir el acceso remoto a la transmisión en vivo o a las configuraciones de la cámara. Usar este dork puede exponer cámaras que no cuentan con contraseñas o configuraciones de seguridad adecuadas, haciéndolas accesibles públicamente.
 
**Listar bases de datos accesibles**

- _Dork:_ `intext:"phpMyAdmin" "Welcome to phpMyAdmin"`  

- Explicación: Este dork busca paneles de administración de phpMyAdmin expuestos, una herramienta común para gestionar bases de datos MySQL. Si no están asegurados adecuadamente, estos paneles pueden permitir acceso directo a bases de datos sensibles.  

**Buscar páginas de error reveladoras**

- Dork: `intext:"Warning: mysql_fetch_array()"`  

- Explicación: Este comando busca mensajes de error relacionados con bases de datos MySQL. Estas páginas de error pueden revelar información técnica útil para un atacante, como la estructura de la base de datos, nombres de tablas o campos, que luego pueden ser explotados.

### <span style="color:purple;">**_WAPPALYZER_**</span>  

**Wappalyzer** es una herramienta que permite identificar las tecnologías y frameworks que utiliza un sitio web. Su propósito es revelar información detallada sobre el software que ejecuta un sitio, como sistemas de gestión de contenido (CMS), lenguajes de programación, bibliotecas JavaScript, servidores web, servicios de análisis, entre otros.  

Esta herramienta es útil para desarrolladores, analistas de seguridad y pentesters porque permite comprender la infraestructura tecnológica de una página web, lo cual es valioso para realizar auditorías, verificar compatibilidades, o detectar posibles vulnerabilidades.

<span style="color: khaki;">**¿Cómo funciona Wappalyzer?**</span>

Wappalyzer inspecciona los encabezados HTTP, los scripts y el código fuente de un sitio web para identificar patrones específicos asociados a ciertas tecnologías. La herramienta utiliza una base de datos con información sobre tecnologías populares y sus características de implementación. A partir de estos patrones, Wappalyzer identifica las tecnologías y presenta un informe detallado.

Wappalyzer está disponible como:

1. **Extensión de navegador** (para Chrome, Firefox, y Edge).
2. **Línea de comandos (CLI)**.
3. **API** para automatizar el análisis a escala.

<span style="color: khaki;">**Ejemplos de Uso de Wappalyzer**</span>

1. **Extensión de Navegador**

Cuando instalas la extensión en tu navegador, puedes visitar cualquier sitio web, y la extensión automáticamente analizará la página y mostrará un resumen de las tecnologías detectadas. Por ejemplo:

- **Sitio:** https://ejemplo.com
- **Resumen de tecnologías:** WordPress (CMS), Google Analytics (Análisis), Nginx (Servidor web), PHP (Lenguaje de programación), MySQL (Base de datos).

2. **Comando CLI para Automatización**

Usar Wappalyzer en la terminal permite realizar un análisis masivo de varios sitios a la vez. Ejemplo de comando en CLI:

```wappalyzer https://ejemplo.com```


3. **API de Wappalyzer**  

La API permite integrarlo en scripts para verificar las tecnologías usadas en varios sitios automáticamente. Esta opción es útil para realizar auditorías de seguridad o para proyectos de investigación.  

### <span style="color:purple;">**DESCUBRIENTOS AUTOMATIZADOS: _FFUF, DIR, GOSBUSTER_**</span>  

<span style="color: khaki;">**¿Qué es el descubrimiento automatizado?**</span>

El descubrimiento automático es el proceso de utilizar herramientas para descubrir contenido en lugar de hacerlo manualmente. Este proceso está automatizado, ya que normalmente contiene cientos, miles o incluso millones de solicitudes a un servidor web. Estas solicitudes comprueban si existe un archivo o directorio en un sitio web, lo que nos da acceso a recursos que antes no sabíamos que existían. Este proceso es posible gracias al uso de un recurso llamado _WORDLISTS_.

<span style="color: khaki;">**¿Qué son las _Wordlists_?**</span>

Las _wordlists_ son simplemente archivos de texto que contienen una larga lista de palabras de uso común; pueden cubrir muchos casos de uso diferentes. Por ejemplo, una lista de palabras de contraseñas incluiría las contraseñas más utilizadas, mientras que en nuestro caso estamos buscando contenido, por lo que necesitaríamos una lista que contenga los nombres de directorios y archivos más utilizados. Un excelente recurso que recomiendo instalar es https://github.com/danielmiessler/SecLists, incluye wordlists de todo tipo.

<span style="color: khaki;">**FFUF**</span>

Ffuf (Fuzz Faster U Fool) es una herramienta de fuerza bruta rápida para fuzzing de contenido web, como directorios, archivos y parámetros. Su velocidad y flexibilidad la hacen ideal para encontrar rutas ocultas, archivos, subdominios y recursos web.

### **Buscar directorios y archivos ocultos en un sitio web:**  

`ffuf -u https://ejemplo.com/FUZZ -w /ruta/a/diccionario.txt`  

- **-u:** Especifica la URL, donde "FUZZ" es el lugar que ffuf probará con cada palabra del diccionario.  

- **-w:** Define el diccionario con palabras para fuzzing.

### **Buscar parámetros GET vulnerables o no documentados:**  

> Los parámetros GET son datos que se envían en la URL de una solicitud HTTP cuando un usuario accede a una página web o realiza una búsqueda.

`ffuf -u https://ejemplo.com/page.php?FUZZ=valor -w /ruta/a/diccionario.txt`

Ffuf tomará cada palabra de la wordlists y la insertará en la posición de FUZZ en la URL. Por ejemplo, si el diccionario contiene palabras como id, token, y session, ffuf generará solicitudes a:

- https://ejemplo.com/page.php?id=valor
- https://ejemplo.com/page.php?token=valor
- https://ejemplo.com/page.php?session=valor

### **Detectar subdominios ocultos de un sitio web:**

`ffuf -u https://FUZZ.ejemplo.com -w /ruta/a/diccionario_subdominios.txt -H "Host: FUZZ.ejemplo.com"`

- **-u:** Especifica la URL, donde "FUZZ" es el lugar que ffuf probará con cada palabra del diccionario.  

- **-w:** Define el diccionario con palabras para fuzzing.  

- **-H:** agrega un encabezado HTTP personalizado a la solicitud.  
 
- **"Host: FUZZ.ejemplo.com"** indica al servidor que la solicitud va dirigida a FUZZ.ejemplo.com. Esto es importante en caso de que el servidor requiera un encabezado Host específico para redirigir correctamente la solicitud al subdominio probado.  

> El encabezado Host asegura que el servidor entienda el subdominio que estás intentando explorar. Esto es vital en la búsqueda de subdominios  

<span style="color: khaki;">**DIRB**</span>

Dirb no se especializa en descubrir subdominios ni parámetros GET. Su función principal es escanear directorios y archivos en rutas específicas de un dominio que se haya proporcionado, utilizando un diccionario de palabras para buscar rutas o archivos potenciales en el sitio web.  

### **Buscar directorios y archivos ocultos en un sitio web:**  

`dirb https://ejemplo.com /ruta/a/diccionario.txt`  

Este es el comando más sencillo pero muy util, ademas, hay otros argumentos adicionales para hacer el escaneo mas eficaz o exacto:  

- **`-r`**: Evitar que se sigan redirecciones. Útil si quieres ver qué redirecciones se encuentran sin ser llevado automáticamente a la nueva URL.  

- **`-x .php`**: Escanear solo archivos de un tipo específico. Si buscas archivos específicos como .php, .html, .txt, puedes especificarlo para reducir el tiempo de búsqueda.  

- **`-f`**: Forzar mostrar códigos HTTP 403/401 (acceso prohibido/no autorizado). Los códigos 403/401 normalmente son ignorados en un escaneo básico, pero forzar su inclusión puede ser útil para detectar rutas restringidas.

- **`-S`**: No mostrar respuestas con código 404 (No encontrado). Para obtener solo rutas que existen, ignorando las que devuelvan error 404.

- **`-p`**: Especificar un proxy. Útil si estás usando un proxy para monitorear el tráfico, como Burp Suite.VV

<span style="color: khaki;">**GOBUSTER**</span>

Gobuster es una herramienta de fuerza bruta diseñada para descubrir directorios y archivos ocultos en un servidor web, así como subdominios, mediante la ejecución de ataques basados en diccionarios.  

Utiliza la velocidad de `Go` para ejecutar solicitudes rápidamente, lo que lo hace útil en auditorías de seguridad para identificar rutas y recursos no documentados en sitios web.  

> La "velocidad de Go" se refiere a Go (o Golang), el lenguaje de programación en el que está escrito gobuster. Go es un lenguaje desarrollado por Google que es conocido por ser muy eficiente y rápido. La concurrencia en Go permite que gobuster envíe muchas solicitudes simultáneamente sin ralentizarse

### **Escaneo de directorios y archivos ocultos en una URL:**  

`gobuster dir -u https://ejemplo.com -w /ruta/a/diccionario.txt`

- **`dir`**: Indica que queremos buscar directorios y archivos.  

- **`-u`**: La URL objetivo.  

- **`-w`**: El diccionario que se usará para la búsqueda.  

### **Escaneo de subdominios:**  

`gobuster dns -d ejemplo.com -w /ruta/a/diccionario_subdominios.txt`  

- **`dns`**: Modo para escanear subdominios.  

- **`-d`**: Dominio base para buscar subdominios.

- **`-w`**: Diccionario para la búsqueda de subdominios.

### **Argumentos adicionales para ejecutar _Gobuster_**  

- **`-r`**: Evitar que se sigan redirecciones. Útil si quieres ver qué redirecciones se encuentran sin ser llevado automáticamente a la nueva URL.  

- **`-x .php`**: Escanear solo archivos de un tipo específico. Si buscas archivos específicos como .php, .html, .txt, puedes especificarlo para reducir el tiempo de búsqueda.  

- **`-f`**: Forzar mostrar códigos HTTP 403/401 (acceso prohibido/no autorizado). Los códigos 403/401 normalmente son ignorados en un escaneo básico, pero forzar su inclusión puede ser útil para detectar rutas restringidas.

- **`-S`**: No mostrar respuestas con código 404 (No encontrado). Para obtener solo rutas que existen, ignorando las que devuelvan error 404.

- **`-p`**: Especificar un proxy. Útil si estás usando un proxy para monitorear el tráfico, como Burp Suite.  

- **`-t <n>`**: Especificar el número de hilos. Permite aumentar la velocidad del escaneo al realizar múltiples solicitudes simultáneamente. Por ejemplo, -t 50 enviará 50 solicitudes al mismo tiempo.  

## <span style="color:blue;">**ENUMERACION DE SUBDOMINIOS**</span> 